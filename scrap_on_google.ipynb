{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time recorded in the file is :  1617634135.1432438\n",
      "The time now  is :  1617634947.0794308\n",
      "A total difference of 13.532269783814748 minutes\n",
      "It has been less than 15 minutes since the proxies were renewed, therefore sticking with the old proxies\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import os, re, magic, json, sys\n",
    "import time, urllib\n",
    "from googlesearch import search\n",
    "from datetime import datetime, timedelta\n",
    "from pymongo import MongoClient\n",
    "from search_engine_scraper import google_search,bing_search,yahoo_search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import logging, logging.handlers\n",
    "import random\n",
    "import subprocess\n",
    "import signal, unicodedata\n",
    "import html2text\n",
    "from contextlib import contextmanager\n",
    "from bs4 import BeautifulSoup, UnicodeDammit\n",
    "import pprint\n",
    "import PyPDF2\n",
    "from time import mktime, strptime\n",
    "from datetime import datetime\n",
    "import json\n",
    "import spacy\n",
    "from ipynb.fs.full.similarity_measure import compute_best_doc\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "#def constant():\n",
    "CHEMIN_LOG = \"./log/\"\n",
    "CHEMIN_RESULTATS = \"./documents/\"\n",
    "NOT_SITES = \"-site:youtube.com -site:pagesjaunes.fr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log(object):\n",
    "    def __init__(self, dossier, nomFichier, niveau=logging.DEBUG):\n",
    "        super(Log, self).__init__()\n",
    "\n",
    "        self.__logger__ = logging.getLogger(nomFichier)\n",
    "        self.__logger__.setLevel(niveau)\n",
    "\n",
    "        format = logging.Formatter('%(asctime)s :: %(levelname)s :: %(message)s', datefmt='%d-%m-%Y %H:%M:%S')\n",
    "        fichierLog = logging.handlers.RotatingFileHandler(\"{0}{1}.log\".format(dossier, nomFichier), 'a', 1000000, 1)\n",
    "\n",
    "        fichierLog.setLevel(niveau)\n",
    "        fichierLog.setFormatter(format)\n",
    "        self.__logger__.addHandler(fichierLog)\n",
    "\n",
    "        console = logging.StreamHandler()\n",
    "        console.setLevel(niveau)\n",
    "        self.__logger__.addHandler(console)\n",
    "\n",
    "    def info(self, message):\n",
    "        self.__logger__.info(message)\n",
    "\n",
    "    def debug(self, message):\n",
    "        self.__logger__.debug(message)\n",
    "\n",
    "    def warning(self, message):\n",
    "        self.__logger__.warning(message)\n",
    "\n",
    "    def error(self, message):\n",
    "        self.__logger__.error(message)\n",
    "\n",
    "    def critical(self, message):\n",
    "        self.__logger__.critical(message)\n",
    "\n",
    "    def exception(self, message):\n",
    "        self.__logger__.exception(message)\n",
    "\n",
    "    def close(self):\n",
    "        for handler in  self.__logger__.handlers[:] :\n",
    "            handler.close()\n",
    "            self.__logger__.removeHandler(handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pause(logger, minutes=0.5):\n",
    "    \"\"\"\n",
    "\t\tEffectue une pause.\n",
    "\n",
    "\t\t@type  log: Logger.\n",
    "\t\t@param log: Fichier de log.\n",
    "\n",
    "\t\t@type  minutes: Entier.\n",
    "\t\t@param minutes: Temps en minute à attendre.\n",
    "\t\"\"\"\n",
    "    date = datetime.now().strftime('%d-%m-%Y:%Hh%M')\n",
    "    current_time = datetime.now() + timedelta(minutes=minutes)\n",
    "\n",
    "    logger.info(\"{0} : Nombre limite de requete atteint. Reprise du programme : {1}\".format(date, current_time.strftime(\n",
    "        '%d-%m-%Y:%Hh%M')))\n",
    "\n",
    "    while datetime.now() < current_time:\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_requetes_(ville, motscles, logger, site):\n",
    "    \n",
    "    logger.info(\"Génération des requêtes\")\n",
    "\n",
    "    requetes_effectuees = []\n",
    "\n",
    "    with open(\"{0}{1}/.sauvegarde.txt\".format(CHEMIN_RESULTATS, ville)) as fichier:\n",
    "        requetes_effectuees = fichier.readlines()\n",
    "\n",
    "    for i in range(0, len(motscles), 1):\n",
    "        # motscles_couple = motscles[i].split(\"+\")\n",
    "\n",
    "        # Si aucun site n'est spécifié alors on n'exclut\n",
    "        # seulement ceux définis dans les constants\n",
    "        if site == \"\":\n",
    "            site_or_not_sites = NOT_SITES\n",
    "        # Sinon on cherche uniquement sur le site spécifié\n",
    "        else:\n",
    "            site_or_not_sites = \"site:\" + site\n",
    "        requete = \"\\\"{0}\\\" AND {1}\".format(ville,motscles[i])\n",
    "        if not any(requete in s for s in requetes_effectuees):\n",
    "            yield requete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertion_document(document, collection):\n",
    "    \"\"\" Insère un document (dictionnaire) dans une collection MongoDB \"\"\"\n",
    "    resultat = collection.insert_one(document)\n",
    "    return resultat.inserted_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_to_iso_date(pdfdate): \n",
    "    datestring = ''\n",
    "    if pdfdate !='':\n",
    "        if len(pdfdate)==23:\n",
    "            datestring = pdfdate[2:-7]\n",
    "        elif len(pdfdate)==17:\n",
    "            datestring = pdfdate[2:-1] \n",
    "        elif len(pdfdate)==21:\n",
    "            datestring = pdfdate[:-7]\n",
    "        ts = strptime(datestring,\"%Y%m%d%H%M%S\")# \"%Y-%m-%dT%H:%M:%S.%fZ\" )# \n",
    "        #print(ts)\n",
    "        dt = datetime.fromtimestamp(mktime(ts))\n",
    "        #print(dt)\n",
    "        #new_format = dt.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")# ('%m-%d-%Y')\n",
    "        new_format = dt.isoformat()\n",
    "    else:\n",
    "        new_format = \"no_date\"\n",
    "    return new_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def get_pdf_info(f):\\n    #pp = pprint.PrettyPrinter(indent=4)\\n    fd = PyPDF2.PdfFileReader(f, 'rb')\\n    doc_info = fd.getDocumentInfo()\\n    #pp.pprint(doc_info)\\n    return format_to_iso_date(doc_info['/ModDate'])\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def get_pdf_info(f):\n",
    "    #pp = pprint.PrettyPrinter(indent=4)\n",
    "    fd = PyPDF2.PdfFileReader(f, 'rb')\n",
    "    doc_info = fd.getDocumentInfo()\n",
    "    #pp.pprint(doc_info)\n",
    "    return format_to_iso_date(doc_info['/ModDate'])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_info(f):\n",
    "    #pp = pprint.PrettyPrinter(indent=4)\n",
    "    fd = PyPDF2.PdfFileReader(f, 'rb')\n",
    "    doc_info = fd.getDocumentInfo()\n",
    "    if '/ModDate' in doc_info:\n",
    "        val_ = doc_info['/ModDate']\n",
    "    elif '/CreationDate' in doc_info:\n",
    "        val_ = doc_info['/CreationDate']\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return format_to_iso_date(val_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract spatial named entities\n",
    "def SNE_Extract(title_mtd):\n",
    "    SNE = {}\n",
    "    nlp = spacy.load('fr_core_news_sm')# Text with nlp\n",
    "    dc = nlp(title_mtd)\n",
    "    i = 0\n",
    "    for ent in dc.ents:\n",
    "        if ent.label_ in ['LOC']:\n",
    "            SNE['ent'+repr(i)] = repr(ent)\n",
    "            i+=1\n",
    "    return SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cette fonction ne prend que la date de publis\n",
    "#à adapter pour prendre en compte l'extraction auto d'entitées nommées temporelles dans les titres\n",
    "def TNE_extract(mtd): #ajouter title_mtd pour la prise en compte des ENT presentent dans le titles\n",
    "    mtd['TNE'] = {}\n",
    "    if 'post_date' in mtd and '$date' in mtd['post_date']:        \n",
    "        mtd['TNE']['date'] = mtd['post_date']['$date']\n",
    "    return mtd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajout de meta données suplementaires pour l'intdexation spatiale et temporelle\n",
    "def enrich_mtd(mtd):\n",
    "    if \"title\" in mtd:\n",
    "        title_mtd = mtd['title']\n",
    "        SNE = SNE_Extract(title_mtd)\n",
    "        mtd['SNE'] = SNE #liste d'ENS\n",
    "        \n",
    "    mtd['TNE'] = {}\n",
    "    #if 'post_date' in mtd and '$date' in mtd['post_date']:        \n",
    "    #    mtd['TNE']['date'] = mtd['post_date']['$date']\n",
    "    if \"post_date\" in mtd: \n",
    "        mtd['TNE']['date'] = []\n",
    "        mtd['TNE']['date'] = mtd['post_date']\n",
    "    #mtd = TNE_extract(mtd)\n",
    "    return mtd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quelques étapes de preprocess\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "class TimeoutException(Exception): pass\n",
    "\n",
    "@contextmanager\n",
    "def limitation_temporelle(seconds):\n",
    "    def signal_handler(signum, frame):\n",
    "        raise TimeoutException\n",
    "\n",
    "    signal.signal(signal.SIGALRM, signal_handler)\n",
    "    signal.alarm(seconds)\n",
    "\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "\n",
    "def norm_string(string):\n",
    "\t\"\"\" On supprime les accents et on remplace les caractères spéciaux par un tiret bas \"\"\"\n",
    "\tnew_string = unicodedata.normalize('NFKD', string).encode('ascii', 'ignore').decode('utf-8')\n",
    "\treturn re.sub(r'[^\\w\\[\\]\\./]+', '_', new_string)\n",
    "\n",
    "def formatage_mots(mot):\n",
    "    \"\"\"\n",
    "        Supprime les retour charriot et remplace les espace par un ?.\n",
    "\n",
    "        @type  mot: String.\n",
    "        @param mot: Mot à formaté.\n",
    "\n",
    "        @rtype: String.\n",
    "        @return: Mot formaté.\n",
    "    \"\"\"\n",
    "    liste_caracteres = dict()\n",
    "\n",
    "    liste_caracteres[\"\\n\"] = \"\"\n",
    "    liste_caracteres[\" \"] = \"?\"\n",
    "\n",
    "    for i in liste_caracteres :\n",
    "        mot = mot.replace(i, liste_caracteres[i])\n",
    "\n",
    "    return mot\n",
    "\n",
    "def get_paire(ligne):\n",
    "    \"\"\"\n",
    "        Extrait du texte la clé et la valeur.\n",
    "\n",
    "        @type  ligne: String.\n",
    "        @param ligne: Texte contenant les valeurs.\n",
    "\n",
    "        @rtype: Tuple.\n",
    "        @return: Valeurs extraites.\n",
    "    \"\"\"\n",
    "    cle, sep, valeur = ligne.strip().partition(\"=\")\n",
    "    return cle.replace(\" \",\"\"), valeur.replace(\" \",\"\")\n",
    "\n",
    "def creation_dossier_resultat(chemin_resultats, ville, log, keyword_file) :\n",
    "    \"\"\"\n",
    "        Création des dossiers de stockage et renvoi des mots clés.\n",
    "\n",
    "        @type  ville: String.\n",
    "        @param ville: Nom de la ville.\n",
    "\n",
    "        @rtype: Liste de string.\n",
    "        @return: Liste des mots clés à rechercher.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(chemin_resultats + ville):\n",
    "        os.makedirs(chemin_resultats + ville)\n",
    "        log.info(\"Création des dossier pour {0}\".format(ville))\n",
    "    if not os.path.exists(\"{0}{1}/Documents_SRC\".format(chemin_resultats, ville)):\n",
    "        os.makedirs(\"{0}{1}/Documents_SRC\".format(chemin_resultats, ville))\n",
    "        log.info(\"Création du dossier Documents_SRC\\n\")\n",
    "\n",
    "    with open(keyword_file) as keywords_file:\n",
    "        words = keywords_file.readlines()\n",
    "        log.info(\"Lecture de keywords.txt ... \\n\")\n",
    "\n",
    "    open(\"{0}{1}/.sauvegarde.txt\".format(chemin_resultats, ville),\"a\").close()\n",
    "    for i in range(0, len(words)):\n",
    "        words[i] = formatage_mots(words[i])\n",
    "    sampling = random.choices(words, k=3)\n",
    "    #print('>>',words)\n",
    "    #print('>>>',sampling)\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "def extract_date(text):\n",
    "    # Motif : JJ/MM/AAAA\n",
    "    match_date = re.search(r'(\\d{2})/(\\d{2})/(\\d{4})', text)\n",
    "    # Motif : JJ mois AAAA\n",
    "    match_date2 = re.search(r'(\\d{1,2}) (\\w+) (\\d{4})', text)\n",
    "\n",
    "    if match_date:\n",
    "        date = datetime(\n",
    "            int(match_date[3]), # Année\n",
    "            int(match_date[2]), # Mois\n",
    "            int(match_date[1])  # Jour\n",
    "        )\n",
    "\n",
    "        return date\n",
    "\n",
    "    elif match_date2:\n",
    "        mapping_month = {\n",
    "            'janvier': 1,\n",
    "            'février': 2,\n",
    "            'mars': 3,\n",
    "            'avril': 4,\n",
    "            'mai': 5,\n",
    "            'juin': 6,\n",
    "            'juillet': 7,\n",
    "            'août': 8,\n",
    "            'septembre': 9,\n",
    "            'octobre': 10,\n",
    "            'novembre': 11,\n",
    "            'décembre': 12,\n",
    "        }\n",
    "\n",
    "        month = match_date2[2].lower()\n",
    "\n",
    "        if month in mapping_month:\n",
    "            date = datetime(\n",
    "                int(match_date2[3]), # Année\n",
    "                mapping_month[month], # Mois\n",
    "                int(match_date2[1])  # Jour\n",
    "            )\n",
    "\n",
    "            return date\n",
    "\n",
    "def html_date_to_isoformat(date):\n",
    "    #x = \"2020-09-18T00:00:00.000Z\"\n",
    "    ts = strptime(date,\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    dt = datetime.fromtimestamp(mktime(ts))\n",
    "    return dt.isoformat()\n",
    "\n",
    "def less_html(html_doc):\n",
    "    \"\"\" Prend du code HTML en entrée et retourne un code épuré de certaines balises \"\"\"\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    # Regex pour matcher les attributs contenant ces termes\n",
    "    bad_classes = re.compile(r'menu|head|publici|share|social|button|alert|prev|next|foot|tags|label|sidebar|author|topics|contact|modal|nav|snippet|register|aside|logo|bandeau|immobilier', re.IGNORECASE)\n",
    "    # Suppression des espaces ou des sauts de ligne au début et à la fin du titre\n",
    "    title = re.sub(r'^\\s|\\s$', '', soup.find('title').text)\n",
    "\n",
    "    # Dictionnaire des métadonnées\n",
    "    metadata = {}\n",
    "    metadata['title'] = title\n",
    "    # Recherche d'une éventuelle balise dont la classe contient \"date\"\n",
    "    # En principe la première date est la date de publication\n",
    "    bloc_date = soup.find(class_=re.compile(r'date', re.IGNORECASE))\n",
    "\n",
    "    if bloc_date:\n",
    "        # Recherche du premier motif JJ/MM/AAAA,\n",
    "        # en principe la date de publication\n",
    "        metadata['post_date'] = ''\n",
    "        date = extract_date(bloc_date.text)\n",
    "        if date: metadata['post_date'] = html_date_to_isoformat(date)\n",
    "\n",
    "    for balise in soup.find_all():\n",
    "        conditions = (\n",
    "            balise.name == 'head',\n",
    "            balise.name == 'nav',\n",
    "            balise.name == 'footer',\n",
    "            balise.name == 'aside',\n",
    "            balise.name == 'script',\n",
    "            balise.name == 'style',\n",
    "            balise.name == 'a',\n",
    "            balise.name == 'figure',\n",
    "            balise.name == 'img',\n",
    "            balise.name == 'svg',\n",
    "            balise.name == 'noscript',\n",
    "            balise.name == 'form',\n",
    "            balise.name == 'button'\n",
    "        )\n",
    "\n",
    "        if any(conditions):\n",
    "            balise.extract()\n",
    "        # On ajoute un espace devant chaque span, pour éviter\n",
    "        # parfois d'avoir des mots collés\n",
    "        elif balise.name == 'span' and balise.string:\n",
    "            balise.string = ' ' + balise.string\n",
    "\n",
    "    for balise in soup.find_all(attrs={'class': bad_classes}):\n",
    "        balise.decompose()\n",
    "\n",
    "    for balise in soup.find_all(attrs={'id': bad_classes}):\n",
    "        balise.decompose()\n",
    "\n",
    "    for balise in soup.find_all():\n",
    "        if balise.text == '': balise.extract()\n",
    "\n",
    "    return metadata, str(soup)\n",
    "\n",
    "def convert_pdf_to_txt(src_file_path):\n",
    "    \"\"\"\n",
    "        Appel externe à pdftotext.\n",
    "\n",
    "        -q : pas de message d'erreur dans la sortie.\n",
    "         - : envoie la sortie dans la console au lieu d'un fichier texte.\n",
    "\n",
    "        Capture de la sortie texte.\n",
    "\n",
    "        @type  src_file_path: String.\n",
    "        @param src_file_path: Chemin du fichier source.\n",
    "\n",
    "        @rtype: String.\n",
    "        @return: Texte brut.\n",
    "    \"\"\"\n",
    "    completed_process = subprocess.run([\"pdftotext\", \"-q\", src_file_path, \"-\"], stdout=subprocess.PIPE)\n",
    "    return completed_process.stdout.decode('utf-8')\n",
    "\n",
    "\n",
    "def convert_html_to_txt(src_file_path):\n",
    "    \"\"\"\n",
    "        Conversion à l'aide de html2text.\n",
    "        Détection automatique de l'encodage (UnicodeDammit).\n",
    "        On capture la sortie texte.\n",
    "\n",
    "        @type  src_file_path: String.\n",
    "        @param src_file_path: Chemin du fichier source.\n",
    "\n",
    "        @rtype: String.\n",
    "        @return: Texte brut.\n",
    "    \"\"\"\n",
    "    html_file = open(src_file_path, 'rb').read()\n",
    "    dammit = UnicodeDammit(html_file)\n",
    "    metadata, html_mini = less_html(html_file.decode(dammit.original_encoding))\n",
    "\n",
    "    handler = html2text.HTML2Text()\n",
    "    handler.ignore_links = True\n",
    "    handler.ignore_emphasis = True\n",
    "    text = handler.handle(html_mini)\n",
    "\n",
    "    return metadata, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner():\n",
    "    \"\"\"docstring for TextCleaner.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Expressions régulières \"\"\"\n",
    "        self.match_alpha = re.compile(r'\\w{3,}') # Existence d'un mot d'au moins 3 lettres\n",
    "        self.match_formfeed = re.compile(r'\\f') # Saut de page\n",
    "        self.match_useless_line = re.compile(r'^[^\\w•]+$|^[\\d\\. ]+$', re.MULTILINE) # Ligne ne contenant aucun caractère alphanumérique\n",
    "\n",
    "        self.match_bad_nl = re.compile(r'([^\\.?!\\n:])\\n+(?![IVX\\d]+[\\.\\)]|ANNEXE)([\\w\\(«\\\"=<>])') # Mauvais saut de ligne\n",
    "        self.match_bad_nl2 = re.compile(r'(\\.{4,} ?\\d+) (\\w)') # Mauvais saut de ligne\n",
    "        self.make_paragraph = re.compile(r'\\.\\n(?=\\w)') # On sépare mieux les paragraphes\n",
    "        self.match_toomuch_nl = re.compile(r'\\n{3,}') # Nouvelles lignes surnuméraires\n",
    "        self.match_begin_end_nl = re.compile(r'^\\n+|\\n+$') # Nouvelles lignes au début et à la fin\n",
    "\n",
    "        self.match_begin_end_space = re.compile(r'^[ \\t]+|[ \\t]+$', re.MULTILINE) # Espace ou tabulation en début et fin de ligne\n",
    "        self.match_toomuch_spaces = re.compile(r' {2,}|\\t+') # Espaces surnuméraires et tabulations\n",
    "\n",
    "        self.match_link = re.compile(r'!?\\[.*?\\]\\(.*?\\)|https?://[^ ]+', re.DOTALL) # Lien issu de la conversion depuis HTML\n",
    "        self.match_cesure = re.compile(r'(\\w)-(\\w)') # Césure\n",
    "        self.match_stuckwords = re.compile(r'(/\\d{2,4}|[a-z])([A-ZÉÈÀÔ])') # Dates et mots collés\n",
    "\n",
    "        self.match_odd = re.compile(r'[\u0007\u0003�●§\\\\\u0019\\|]+|\\(cid:\\d+\\)|(?<=- )W ') # caractère bizarre\n",
    "        self.match_accent1 = re.compile(r'Ã©') # é\n",
    "        self.match_accent2 = re.compile(r'Ã¨') # è\n",
    "        self.match_accent3 = re.compile(r'Ã') # à\n",
    "        self.match_puce1 = re.compile(r'')\n",
    "        self.match_puce2 = re.compile(r'[\u0001\u0002]')\n",
    "        self.match_diam = re.compile(r'diam\\.')\n",
    "        self.match_apostrophe = re.compile(r'’')\n",
    "\n",
    "    def clean(self, text):\n",
    "        # Remplacement des espaces insécables\n",
    "        text = text.replace(u'\\xa0', ' ')\n",
    "\n",
    "        text = self.match_link.sub('', text)\n",
    "        text = self.match_formfeed.sub('\\n\\n', text)\n",
    "        text = self.match_useless_line.sub('\\n', text)\n",
    "\n",
    "        text = self.match_diam.sub('diamètre', text)\n",
    "        text = self.match_accent1.sub('é', text)\n",
    "        text = self.match_accent2.sub('è', text)\n",
    "        text = self.match_accent3.sub('à', text)\n",
    "        text = self.match_puce1.sub('*', text)\n",
    "        text = self.match_puce2.sub('-', text)\n",
    "        text = self.match_odd.sub('', text)\n",
    "        text = self.match_apostrophe.sub('\\'', text)\n",
    "\n",
    "        text = self.match_begin_end_space.sub('', text)\n",
    "        text = self.match_bad_nl.sub(r'\\g<1> \\g<2>', text)\n",
    "        # On double la réparation des lignes, meilleurs résultats\n",
    "        text = self.match_bad_nl.sub(r'\\g<1> \\g<2>', text)\n",
    "        text = self.match_bad_nl2.sub(r'\\g<1>\\n\\g<2>', text)\n",
    "        text = self.make_paragraph.sub('.\\n\\n', text)\n",
    "        text = self.match_stuckwords.sub(r'\\g<1> \\g<2>', text)\n",
    "        text = self.match_toomuch_spaces.sub(' ', text)\n",
    "        text = self.match_toomuch_nl.sub('\\n\\n', text)\n",
    "        text = self.match_begin_end_nl.sub('', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def exists_alpha(self, text):\n",
    "        \"\"\" Contrôle l'existence de caractères alphanumériques \"\"\"\n",
    "        return self.match_alpha.search(text) is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sauvegarde_fichier_simpl(ville, url, logger, tc):\n",
    "    \"\"\"\n",
    "\t\tEnregistre un fichier quelqu'il soit.\n",
    "\n",
    "\t\t@type  ville: String.\n",
    "\t\t@param ville: Nom de la ville.\n",
    "\n",
    "\t\t@type  url: String.\n",
    "\t\t@param url: URL vers le fichier pdf.\n",
    "\n",
    "\t\t@type  log: Logger.\n",
    "\t\t@param log: Fichier de log.\n",
    "\n",
    "\t\t@type  tc: TextCleaner.\n",
    "\t\t@param tc: Nettoyeur de texte.\n",
    "\t\"\"\"\n",
    "    # On prend l'URL de base pour désigner l'origine du fichier à laquelle on enlève le \"www.\"\n",
    "    origine = re.sub(r'^www\\.', '', urllib.request.urlparse(url).netloc)\n",
    "    url_split = url.split('/')\n",
    "\n",
    "    # Extraction du nom du document\n",
    "    if url_split[-1] == '':\n",
    "        nom_document = norm_string(url_split[-2])\n",
    "    else:\n",
    "        nom_document = norm_string(url_split[-1])\n",
    "\n",
    "    src_file_path = \"{0}{1}/Documents_SRC/[{2}]{3}\".format(CHEMIN_RESULTATS, ville, origine, nom_document)\n",
    "\n",
    "    if not os.path.isfile(src_file_path):\n",
    "        req = urllib.request.Request(url, headers={'User-Agent': \"Magic Browser\"})\n",
    "        response = urllib.request.urlopen(req)\n",
    "        read_buffer = response.read()\n",
    "        mime_type = magic.from_buffer(read_buffer, mime=True)\n",
    "\n",
    "        source = {\n",
    "            'file_link': src_file_path,\n",
    "            'complete_url': url,\n",
    "            'base_url': origine,\n",
    "            'open_access': False,\n",
    "        }\n",
    "\n",
    "        document = {\n",
    "            'name': nom_document,\n",
    "            'mime_type': mime_type,\n",
    "            'source': source,\n",
    "            'manually_validated': False,\n",
    "        }\n",
    "\n",
    "        # Écriture du fichier téléchargé\n",
    "        with open(src_file_path, \"wb\") as fichier:\n",
    "            fichier.write(read_buffer)\n",
    "            logger.info(\"Document sauvegardé.\")\n",
    "\n",
    "        # On détecte le type du document source pour utiliser la conversion adaptée\n",
    "        # Si le document est un PDF\n",
    "        if mime_type == 'application/pdf':\n",
    "            try:\n",
    "                # Conversion en texte brut\n",
    "                raw_text = convert_pdf_to_txt(src_file_path)\n",
    "                logger.info(\"Document PDF converti en texte brut.\")\n",
    "                p_d = get_pdf_info(src_file_path)\n",
    "                document['text'] = tc.clean(raw_text)\n",
    "                document['post_date'] = p_d\n",
    "                logger.info(\"Texte nettoyé.\\n\")\n",
    "            except:\n",
    "                logger.exception(\"Erreur lors de la conversion en texte du pdf.\\n\")\n",
    "\n",
    "        # Si le document est un HTML (ou autre fichier web)\n",
    "        elif mime_type == 'text/html':\n",
    "            try:\n",
    "                # Conversion en texte brut\n",
    "                metadata, raw_text = convert_html_to_txt(src_file_path)\n",
    "                logger.info(\"Document web converti en texte brut.\")\n",
    "                document['text'] = tc.clean(raw_text)\n",
    "                document.update(metadata)\n",
    "                logger.info(\"Texte nettoyé.\\n\")\n",
    "\n",
    "            except:\n",
    "                logger.exception(\"Erreur lors de la conversion en texte de la page web.\\n\")\n",
    "\n",
    "        else:\n",
    "            logger.info(\"Autre type de document téléchargé.\\n\")\n",
    "        \n",
    "        #document = enrich_mtd(document)\n",
    "\n",
    "        return document\n",
    "\n",
    "    else:\n",
    "        logger.info(\"Le document existe déjà.\\n\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sauvegarde_fichier_advanced(ville, url, logger, tc):\n",
    "    \"\"\"\n",
    "\t\tEnregistre un fichier quelqu'il soit.\n",
    "\n",
    "\t\t@type  ville: String.\n",
    "\t\t@param ville: Nom de la ville.\n",
    "\n",
    "\t\t@type  url: String.\n",
    "\t\t@param url: URL vers le fichier pdf.\n",
    "\n",
    "\t\t@type  log: Logger.\n",
    "\t\t@param log: Fichier de log.\n",
    "\n",
    "\t\t@type  tc: TextCleaner.\n",
    "\t\t@param tc: Nettoyeur de texte.\n",
    "\t\"\"\"\n",
    "    # On prend l'URL de base pour désigner l'origine du fichier à laquelle on enlève le \"www.\"\n",
    "    origine = re.sub(r'^www\\.', '', urllib.request.urlparse(url).netloc)\n",
    "    url_split = url.split('/')\n",
    "\n",
    "    # Extraction du nom du document\n",
    "    if url_split[-1] == '':\n",
    "        nom_document = norm_string(url_split[-2])\n",
    "    else:\n",
    "        nom_document = norm_string(url_split[-1])\n",
    "\n",
    "    src_file_path = \"{0}{1}/Documents_SRC/[{2}]{3}\".format(CHEMIN_RESULTATS, ville, origine, nom_document)\n",
    "\n",
    "    if not os.path.isfile(src_file_path):\n",
    "        req = urllib.request.Request(url, headers={'User-Agent': \"Magic Browser\"})\n",
    "        response = urllib.request.urlopen(req)\n",
    "        read_buffer = response.read()\n",
    "        mime_type = magic.from_buffer(read_buffer, mime=True)\n",
    "\n",
    "        source = {\n",
    "            'file_link': src_file_path,\n",
    "            'complete_url': url,\n",
    "            'base_url': origine,\n",
    "            'open_access': False,\n",
    "        }\n",
    "\n",
    "        document = {\n",
    "            'name': nom_document,\n",
    "            'mime_type': mime_type,\n",
    "            'source': source,\n",
    "            'manually_validated': False,\n",
    "        }\n",
    "\n",
    "        # Écriture du fichier téléchargé\n",
    "        with open(src_file_path, \"wb\") as fichier:\n",
    "            fichier.write(read_buffer)\n",
    "            logger.info(\"Document sauvegardé.\")\n",
    "\n",
    "        # On détecte le type du document source pour utiliser la conversion adaptée\n",
    "        # Si le document est un PDF\n",
    "        if mime_type == 'application/pdf':\n",
    "            try:\n",
    "                # Conversion en texte brut\n",
    "                raw_text = convert_pdf_to_txt(src_file_path)\n",
    "                logger.info(\"Document PDF converti en texte brut.\")\n",
    "                p_d = get_pdf_info(src_file_path)\n",
    "                document['text'] = tc.clean(raw_text)\n",
    "                document['post_date'] = p_d\n",
    "                logger.info(\"Texte nettoyé.\\n\")\n",
    "            except:\n",
    "                logger.exception(\"Erreur lors de la conversion en texte du pdf.\\n\")\n",
    "\n",
    "        # Si le document est un HTML (ou autre fichier web)\n",
    "        elif mime_type == 'text/html':\n",
    "            try:\n",
    "                # Conversion en texte brut\n",
    "                metadata, raw_text = convert_html_to_txt(src_file_path)\n",
    "                logger.info(\"Document web converti en texte brut.\")\n",
    "                document['text'] = tc.clean(raw_text)\n",
    "                document.update(metadata)\n",
    "                logger.info(\"Texte nettoyé.\\n\")\n",
    "\n",
    "            except:\n",
    "                logger.exception(\"Erreur lors de la conversion en texte de la page web.\\n\")\n",
    "\n",
    "        else:\n",
    "            logger.info(\"Autre type de document téléchargé.\\n\")\n",
    "        \n",
    "        document = enrich_mtd(document)\n",
    "\n",
    "        return document\n",
    "\n",
    "    else:\n",
    "        logger.info(\"Le document existe déjà.\\n\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def run_requete(search):\n",
    "    page = requests.get(f\"https://www.google.com/search?q={search}\")\n",
    "    soup = BeautifulSoup(page.content, \"html5lib\")\n",
    "    links = soup.findAll(\"a\")\n",
    "    url_list = []\n",
    "    for link in links :\n",
    "        print(link)\n",
    "        print(\"QQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQ\")\n",
    "        link_href = link.get('href')\n",
    "        if \"url?q=\" in link_href:# and not \"webcache\" in link_href:\n",
    "            # print (link.get('href').split(\"?q=\")[1].split(\"&sa=U\")[0])\n",
    "            url_list.append(link.get('href').split(\"?q=\")[1].split(\"&sa=U\")[0])\n",
    "    return url_list[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recherche_web_simpl(thematic_keyword, motscles, logger, site):\n",
    "    \"\"\"\n",
    "\t\tEffecue la recherche.\n",
    "\n",
    "\t\t@type  ville: String.\n",
    "\t\t@param ville: Nom de la ville.\n",
    "\n",
    "\t\t@type  motscles: Liste de string.\n",
    "\t\t@param motscles: Liste des mots clés à rechercher.\n",
    "\n",
    "\t\t@type  logger: Logger.\n",
    "\t\t@param logger: Fichier de log.\n",
    "\t\"\"\"\n",
    "    # Connexion à MongoDB\n",
    "    client = MongoClient()\n",
    "    db = client['inventaire_medo']  # Sélection de la base de données\n",
    "    collection = db['agriculture_git']  # Sélection de la collection\n",
    "\n",
    "    # Initialisation du nettoyeur de texte\n",
    "    tc = TextCleaner()\n",
    "    # Génération de la liste des requêtes\n",
    "    liste_requetes = generation_requetes_(formatage_mots(thematic_keyword), motscles, logger, site)\n",
    "    #print('>>#', liste_requetes)\n",
    "    # Date du début de la requête\n",
    "    # date_collecte = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    #date_collecte = datetime.now()\n",
    "\n",
    "    for requete in liste_requetes:\n",
    "        #print('###', requete)\n",
    "        response = search(requete, lang='fr', stop=10) # stop=10\n",
    "        #response = run_requete(requete)  # top@10 first pages\n",
    "        # response = bing_search.search(requete)  # stop=10\n",
    "        try:\n",
    "            #liste_url = list(response)\n",
    "            liste_url = set(response)\n",
    "        except:\n",
    "            liste_url = []\n",
    "        with (open(\"{0}{1}/resume.txt\".format(CHEMIN_RESULTATS, thematic_keyword), \"a\")) as res:\n",
    "            logger.info(\"Nouvelle requête : {0}\\n\".format(requete))\n",
    "\n",
    "            res.write(\"Requête : {0}\\n\".format(requete))\n",
    "            res.write(\"Nombre de résultats affichés : {0}\\n\".format(len(liste_url)))\n",
    "            res.write(\"\\nListe des résultats\\n\")\n",
    "            res.write(\"\\n\")\n",
    "\n",
    "            for url in liste_url:\n",
    "                if not url.endswith('xml.gz'):\n",
    "                    logger.info(\"URL : {0}\\n\".format(url))\n",
    "                    res.write(\"{0}\\n\".format(url))\n",
    "                    try:\n",
    "                        # On temporise chaque requête Google pour ne pas être bloqué\n",
    "                        with limitation_temporelle(30):                       \n",
    "                            document = sauvegarde_fichier_simpl(thematic_keyword, url, logger, tc)\n",
    "                            # On s'assure que le document existe et qu'un texte y est associé\n",
    "                            if document and 'text' in document:\n",
    "                                \n",
    "                                document['source']['type'] = 'web_request'\n",
    "                                document['source']['raw_request'] = requete\n",
    "                                #document['collection_date'] = date_collecte\n",
    "                                document['thematic'] = thematic_keyword\n",
    "                                logger.info(\"Document inséré dans l'inventaire.\\n\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        logger.info(\"Erreur pour l'URL : {0}\\n\".format(url))\n",
    "                        res.write(\"Erreur pour l'URL : {0}\\n\".format(url))\n",
    "\n",
    "            logger.info(\"***********************************************************************\\n\")\n",
    "            res.write(\"***********************************************************************\\n\\n\")\n",
    "\n",
    "        with open(\"{0}{1}/.sauvegarde.txt\".format(CHEMIN_RESULTATS, thematic_keyword), \"a\") as f:\n",
    "            f.write(\"{0}\\n\".format(requete))\n",
    "\n",
    "        pause(logger)\n",
    "        \n",
    "        #score_cps_and_mrgd_syno_terms_sim(document['text'], corpus_file_syno)\n",
    "        return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recherche_web_advanced(spatial_extent,voc_concept, motscles, logger, site):\n",
    "    \"\"\"\n",
    "\t\tEffecue la recherche.\n",
    "\n",
    "\t\t@type  ville: String.\n",
    "\t\t@param ville: Nom de la ville.\n",
    "\n",
    "\t\t@type  motscles: Liste de string.\n",
    "\t\t@param motscles: Liste des mots clés à rechercher.\n",
    "\n",
    "\t\t@type  logger: Logger.\n",
    "\t\t@param logger: Fichier de log.\n",
    "\t\"\"\"\n",
    "    # Connexion à MongoDB\n",
    "    client = MongoClient()\n",
    "    db = client['inventaire_medo']  # Sélection de la base de données\n",
    "    collection = db['agriculture_git']  # Sélection de la collection\n",
    "\n",
    "    # Initialisation du nettoyeur de texte\n",
    "    tc = TextCleaner()\n",
    "\n",
    "    # Liste des stopwords\n",
    "    #stopwords = creer_liste_stopwords('listes_stopwords/stopwords_base.txt', 'listes_stopwords/stopwords_1000.txt')\n",
    "\n",
    "    # Génération de la liste des requêtes\n",
    "    liste_requetes = generation_requetes_(formatage_mots(spatial_extent), motscles, logger, site)\n",
    "    #print('>>#', liste_requetes)\n",
    "    # Date du début de la requête\n",
    "    # date_collecte = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    #date_collecte = datetime.now()\n",
    "\n",
    "    for requete in liste_requetes:\n",
    "        #print('###', requete)\n",
    "        response = search(requete, lang='fr', stop=10) # stop=10\n",
    "        #response = run_requete(requete)  # top@10 first pages\n",
    "        # response = bing_search.search(requete)  # stop=10\n",
    "        try:\n",
    "            #liste_url = list(response)\n",
    "            liste_url = set(response)\n",
    "        except:\n",
    "            liste_url = []\n",
    "        with (open(\"{0}{1}/resume.txt\".format(CHEMIN_RESULTATS, spatial_extent), \"a\")) as res:\n",
    "            logger.info(\"Nouvelle requête : {0}\\n\".format(requete))\n",
    "\n",
    "            res.write(\"Requête : {0}\\n\".format(requete))\n",
    "            res.write(\"Nombre de résultats affichés : {0}\\n\".format(len(liste_url)))\n",
    "            res.write(\"\\nListe des résultats\\n\")\n",
    "            res.write(\"\\n\")\n",
    "\n",
    "            for url in liste_url:\n",
    "                if not url.endswith('xml.gz'):\n",
    "                    logger.info(\"URL : {0}\\n\".format(url))\n",
    "                    res.write(\"{0}\\n\".format(url))\n",
    "                    try:\n",
    "                        # On temporise chaque requête Google pour ne pas être bloqué\n",
    "                        with limitation_temporelle(30):\n",
    "                            \n",
    "                            document = sauvegarde_fichier_advanced(spatial_extent, url, logger, tc)\n",
    "\n",
    "                            # On s'assure que le document existe et qu'un texte y est associé\n",
    "                            if document and 'text' in document:\n",
    "                                compute_best_doc(voc_concept, document)\n",
    "                                document['source']['type'] = 'web_request'\n",
    "                                document['source']['raw_request'] = requete\n",
    "                                #document['collection_date'] = date_collecte\n",
    "                                document['spatial_extent'] = spatial_extent\n",
    "                                # document['textual_genre'] = genre\n",
    "                                document_id = insertion_document(document, collection)\n",
    "                                # logger.info(\"Document classé comme \\\"{}\\\".\".format(genre))\n",
    "                                logger.info(\"Document inséré dans l'inventaire.\\n\")\n",
    "\n",
    "                                # else:\n",
    "                                # os.remove(document['source']['file_link'])\n",
    "                                # logger.info(\"Document non pertinent. Fichier source supprimé.\\n\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        logger.info(\"Erreur pour l'URL : {0}\\n\".format(url))\n",
    "                        res.write(\"Erreur pour l'URL : {0}\\n\".format(url))\n",
    "\n",
    "            logger.info(\"***********************************************************************\\n\")\n",
    "            res.write(\"***********************************************************************\\n\\n\")\n",
    "\n",
    "        with open(\"{0}{1}/.sauvegarde.txt\".format(CHEMIN_RESULTATS, spatial_extent), \"a\") as f:\n",
    "            f.write(\"{0}\\n\".format(requete))\n",
    "\n",
    "        pause(logger)\n",
    "        \n",
    "        #score_cps_and_mrgd_syno_terms_sim(document['text'], corpus_file_syno)\n",
    "        return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some extra to process google data before formating to BioTex inout file\n",
    "def cleanhtml(raw_text, remove_punc=False, lower=False):\n",
    "    \"\"\"\n",
    "    Replace HTML tags in a text.\n",
    "\n",
    "    raw_html : str\n",
    "        html in its raw form\n",
    "    \"\"\"\n",
    "    clean_text = raw_text\n",
    "\n",
    "    # Remove hmtl and url patterns\n",
    "    patterns = [re.compile('<.*?>'), re.compile('\\[\\d\\]'), re.compile('www.\\S+.com')]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        clean_text = re.sub(pattern, '', clean_text)\n",
    "\n",
    "    # Special characters causing pb with Biotex\n",
    "    # ['\\n', '\\t', 'ã', '€', \"\\'\", \"\\xa0\"]\n",
    "    toRemove = ['\\n', '\\t','\\\"', 'ã', '€', \"\\xa0\"]\n",
    "\n",
    "    for char in toRemove:\n",
    "        clean_text = re.sub(char, '', clean_text)\n",
    "\n",
    "    # add whitespace after a dot\n",
    "    rx = r\"\\.(?=\\S)\"\n",
    "    clean_text = re.sub(rx, \". \", clean_text)\n",
    "\n",
    "    if remove_punc:\n",
    "        clean_text = re.sub('[^A-Za-z0-9]+', ' ', clean_text)\n",
    "\n",
    "    if lower:\n",
    "        clean_text = clean_text.lower()\n",
    "\n",
    "    return clean_text.strip()\n",
    "\n",
    "\n",
    "def biotex_corpus_builder(g_corpus,keywords):\n",
    "    #os.chdir(files_dir)\n",
    "    if not os.path.exists(os.getcwd()+'/'+str(keywords)):\n",
    "        os.makedirs(os.getcwd()+'/'+str(keywords))\n",
    "    root = os.getcwd() + '/' + str(keywords)\n",
    "    for doc in g_corpus:\n",
    "        fw = open(root + '/' + str(keywords) + '_google_corpus.txt', 'a+')\n",
    "        fw.write(\"%s\\n\" % cleanhtml(g_corpus['text']))\n",
    "        fw.write(\"\\n##########END##########\\n\")\n",
    "        fw.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_scraper(thematic_keyword,keyword_file, site=''):\n",
    "    logger = Log(CHEMIN_LOG, 'collecteDeDonnees_{0}'.format(datetime.today().strftime(\"%d-%m-%y\")))\n",
    "    # ville = formatage_mots(input(\"Sur quelle ville voulez-vous effectuer la recherche ? \\n\"))\n",
    "    thematic_keyword = thematic_keyword.title()\n",
    "    motscles = creation_dossier_resultat(CHEMIN_RESULTATS, thematic_keyword, logger,keyword_file)\n",
    "    logger.info(\"Début de la recherche de document concernant la ville de {0}\".format(thematic_keyword))\n",
    "    corpus = recherche_web_simpl(thematic_keyword, motscles, logger, site)\n",
    "    #recherche_web_(motscles)\n",
    "    biotex_corpus_builder(corpus,thematic_keyword)\n",
    "    return corpus\n",
    "\n",
    "#main('Montpellier',keyword_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_scraper(spatial_extent,voc_concept, site=''):\n",
    "    logger = Log(CHEMIN_LOG, 'collecteDeDonnees_{0}'.format(datetime.today().strftime(\"%d-%m-%y\")))\n",
    "    # ville = formatage_mots(input(\"Sur quelle ville voulez-vous effectuer la recherche ? \\n\"))\n",
    "    spatial_extent = spatial_extent.title()\n",
    "    motscles = creation_dossier_resultat(CHEMIN_RESULTATS, spatial_extent, logger,voc_concept)\n",
    "    logger.info(\"Début de la recherche de document concernant la ville de {0}\".format(spatial_extent))\n",
    "    recherche_web_advanced(spatial_extent,voc_concept, motscles, logger, site)\n",
    "    #recherche_web_(motscles)\n",
    "\n",
    "#main('Montpellier',keyword_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
