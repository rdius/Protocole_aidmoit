{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import os\n",
    "import json\n",
    "from nltk.corpus import wordnet\n",
    "import glob\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thematic vocabulary concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge and compute mean value of cvalue and tfidfcm\n",
    "def merg_cval_and_tfidfcm_terms(root):\n",
    "    #for file in root:\n",
    "    files = glob.glob(root + '/*.csv',recursive = True) \n",
    "    print(files)\n",
    "    with open(files[0],newline='') as f:\n",
    "        r = csv.reader(f)\n",
    "        data = [line for line in r]\n",
    "        print(len(data))\n",
    "    with open(files[1],newline='') as f:\n",
    "        r1 = csv.reader(f)\n",
    "        data1 = [line for line in r1]\n",
    "        print(len(data1))\n",
    "    with open(root+'/cv_tf.csv','a+',newline='') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(['terms', 'in_umls', 'rank'])\n",
    "        w.writerows(data)\n",
    "        w.writerows(data1)\n",
    "             \n",
    "# rootPath contains all f_dir files\n",
    "def compute_mean_val_of_merged_terms(f_dir):\n",
    "    files = glob.glob(f_dir + '/*cv_tf.csv',recursive = True) \n",
    "    #for file in Path(f_dir).rglob('*cv_tf.csv'):\n",
    "    root, filenam = os.path.split(files[0])\n",
    "    terms = pd.read_csv(files[0], sep=',|;', engine='python')\n",
    "    #print(len(terms))\n",
    "    #filename = file.name\n",
    "    filename = filenam.split('.csv')[0]\n",
    "    #terms = pd.read_csv(file, sep=\",\")\n",
    "    df1 = terms.groupby(['terms'])['rank'].agg([np.mean])\n",
    "    #print(df1)\n",
    "    df1 = df1.sort_values([\"mean\"], ascending=False)\n",
    "    print('final-len', len(df1))\n",
    "    df1.to_csv(root+\"/\"+filename+\"_mean.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read kwds list\n",
    "def read_kwd(txtfile):\n",
    "    f = open(txtfile)\n",
    "    content = f.read()\n",
    "    kw_list = (content.lower()).split('\\n')\n",
    "    return kw_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms semantic similarity evaluation\n",
    "def eval_terms(corpus,queries,top_k):\n",
    "    embedder = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "    corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
    "    D = {}\n",
    "    for query in queries:\n",
    "        D[query] = []\n",
    "        query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "        cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "        cos_scores = cos_scores.cpu()\n",
    "\n",
    "        #We use np.argpartition, to only partially sort the top_k results\n",
    "        top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]\n",
    "\n",
    "        for idx in top_results[0:top_k]:\n",
    "            D[query].append( float('%.4f' % (cos_scores[idx])) )\n",
    "    for val_ in D:\n",
    "        D[val_]= sum(D[val_])/len(D[val_])\n",
    "\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def finalRank_from_BertAnd_Biotex(f_biotex, f_bert):\n",
    "#     d_biotex, d_bert = {}, {}\n",
    "#     df_biotex = pd.read_csv(f_biotex, sep='\\t|,|;', engine='python')\n",
    "#     df_bert = pd.read_csv(f_bert, sep='\\t|,|;', engine='python')\n",
    "\n",
    "#     for i in range(len(df_biotex['terms'])):\n",
    "#         d_biotex[df_biotex['terms'][i]] = 1/(i+1)\n",
    "\n",
    "#     for i in range(len(df_bert['terms'])):\n",
    "#         d_bert[df_bert['terms'][i]] = df_bert['rank'][i]\n",
    "#     D = {}\n",
    "#     for k in d_biotex:\n",
    "#         if k in d_bert:\n",
    "#             Dic[k] = NormalizeData(a[k]+b[k]/2)\n",
    "        \n",
    "#     return D#d_biotex, d_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function for similarity measure\n",
    "def score_mrgd_ts_and_expert_ts_sim(t_file, kwd_file, n_first):\n",
    "    d_biotex = {}\n",
    "    files = glob.glob(t_file + '/*mean.csv',recursive = True) \n",
    "    root, filenam = os.path.split(files[0])\n",
    "    filenam = filenam.split('.csv')[0]\n",
    "\n",
    "    kw_list = read_kwd(kwd_file)\n",
    "    kw_list = kw_list[:-1]\n",
    "\n",
    "    terms = pd.read_csv(files[0], sep='\\t', engine='python')\n",
    "\n",
    "    df = terms['terms']\n",
    "    df = np.array(df)\n",
    "    \n",
    "    df = [word for word in df if (word not in stopwords.words('french'))]\n",
    "    df = [word for word in df if word]\n",
    "    d_bert = eval_terms(kw_list ,df[:1000],n_first)\n",
    "    \n",
    "    ########### use to score with Biotex data pertinence range\n",
    "#     for i in range(len(terms['terms'])):\n",
    "#         d_biotex[terms['terms'][i]] = 1/(i+1)\n",
    "#     print (d_biotex)\n",
    "#     D = {}\n",
    "#     for k in d_biotex:\n",
    "#         if k in d_bert:\n",
    "#             D[k] = NormalizeData(d_biotex[k]+d_bert[k]/2)\n",
    "    ##########\n",
    "    #print(D)\n",
    "    df = pd.DataFrame(list(d_bert.items()), columns=['terms','rank'])\n",
    "    final_df = df.sort_values(by=['rank'], ascending=False)\n",
    "\n",
    "    final_df.to_csv(root+'/'+filenam+'_Bert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_best_terms(biotex_out, expert_concept, n_first):\n",
    "    #root, filenam = os.path.split(biotex_out)\n",
    "    merg_cval_and_tfidfcm_terms(biotex_out)\n",
    "    compute_mean_val_of_merged_terms(biotex_out)\n",
    "    data = score_mrgd_ts_and_expert_ts_sim(biotex_out, expert_concept,n_first)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/rodrique/Bureau/Jupyter-notebook/Protocole_3M/Protocole_aidmoit/biotex_out_/tf_agri.csv', '/home/rodrique/Bureau/Jupyter-notebook/Protocole_3M/Protocole_aidmoit/biotex_out_/cv_agri.csv']\n",
      "127071\n",
      "127087\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final-len 127124\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# biotex_out = \"/home/rodrique/Bureau/Jupyter-notebook/Protocole_3M/Protocole_aidmoit/biotex_out_\"\n",
    "# expert_concept = '/home/rodrique/Bureau/Jupyter-notebook/Protocole_3M/Protocole_aidmoit/expert_concept/agriculture.txt'\n",
    "# n_first = 10\n",
    "# data = compute_best_terms(biotex_out, expert_concept, n_first) # data is in dic format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# biotex_out = \"/home/rodrique/Bureau/Protocole/Corpus_experiment/corpus/agri\"\n",
    "# expert_concept = '/home/rodrique/Bureau/Jupyter-notebook/Protocole_3M/doc_collect_adapted/keywords/agriculture.txt'\n",
    "# data = compute_best_terms(biotex_out, expert_concept, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##usage\n",
    "# definir le chemin d'accès du fichier contenant l'ensemble des termes\n",
    "# indiquer aussi lle chemin du fichier de termes experts de la thematic concernee\n",
    "# indiquer la taille ou nombre de termes à prendre en compte pour le calcule de similarité -n_first-\n",
    "\n",
    "#score_mrgd_ts_and_expert_ts_sim(merged_file, kwd_file,n_first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute doc similarity with extend vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this if corpus is saved with corresponding source data\n",
    "# dic is formated as: {corpus_name:corpus_data}\n",
    "def load_json(file):\n",
    "    with open(file) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_json(dic,des):\n",
    "    with open(des, 'w') as fp:\n",
    "        json.dump(dic, fp)\n",
    "\n",
    "\n",
    "def _format(kwd_list):\n",
    "    nl_ = []\n",
    "    for t in kwd_list:\n",
    "        tmp = t.split(' ')\n",
    "        tmp = [i for i in tmp if i]\n",
    "        for tm in tmp:\n",
    "            nl_.append(tm)\n",
    "    #print(len(nl_))\n",
    "    nl_ = [word for word in nl_ if (word not in stopwords.words('french'))]\n",
    "    nl_ = np.unique(nl_)\n",
    "    #x = \" \".join(k for k in nl_)\n",
    "    return nl_ #x\n",
    "\n",
    "\n",
    "# read keywords file and return each one as sentence\n",
    "def read_kwd_(txtfile):\n",
    "    f = open(txtfile)\n",
    "    content = f.read()\n",
    "    kw_list = (content.lower()).split('\\n')\n",
    "    kw_list = _format(kw_list)\n",
    "    return kw_list\n",
    "\n",
    "\n",
    "# termes similarity scoring with DistilBert\n",
    "# corpus ==> list of extracted terms from the corpus with BioTex\n",
    "# return a dictionary, keys =>  extracted terms, and values => mean of the whole similarity score!\n",
    "# top_k => k\n",
    "def measure_(corpus,query):\n",
    "    embedder = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "    # Corpus with example sentences\n",
    "    corpus_embeddings = embedder.encode(query, convert_to_tensor=True)\n",
    "    #D = {}\n",
    "    query_embedding = embedder.encode(corpus, convert_to_tensor=True)\n",
    "    cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    cos_scores = cos_scores.cpu()\n",
    "    return cos_scores\n",
    "\n",
    "\n",
    "#root = \"/home/rodrique/Bureau/Protocole/test_collecte/tst2\"\n",
    "def score_doc_sim(extend_voc_concept, doc):\n",
    "    #kwd_list = read_kwd_(extend_voc_concept)\n",
    "    kwd_cp = \" \".join(k for k in extend_voc_concept[:1000]) #[:1000]\n",
    "    #print(kwd_cp)\n",
    "    score = measure_(doc[\"text\"],kwd_cp)\n",
    "    doc[\"pertinence\"] = round(score.tolist()[0], 2)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonym(voc_concept):\n",
    "    #Creating a list\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(voc_concept,lang='fra'):#, lang='fra'\n",
    "        for lm in syn.lemmas('fra'):\n",
    "            synonyms.append(lm.name())#adding into synonyms\n",
    "    synonyms = set(synonyms)\n",
    "    return synonyms\n",
    "\n",
    "\n",
    "# List data to csv file\n",
    "def write_list_to_txt(terms_list, outfile):\n",
    "    \"\"\"Write the list to csv file.\"\"\"\n",
    "    with open(outfile, 'w') as f:\n",
    "        for item in terms_list:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    return outfile\n",
    "\n",
    "# recoder pour prendre en compte un fichier csv, la sortie du vocabulaire de concepts final\n",
    "def generate_new_terms(voc_concept):\n",
    "    data = read_kwd(voc_concept)\n",
    "    root, filname = os.path.split(voc_concept)\n",
    "    filname = filname.split('.txt')[0]\n",
    "    L = []\n",
    "    for t in data:\n",
    "        t = t.split(' ')\n",
    "        if t.__len__() == 1:\n",
    "            syn = [get_synonym(a) for a in t]\n",
    "            for v in syn[0]:\n",
    "                L.append(v)\n",
    "        if t.__len__() > 1:\n",
    "            syn = [get_synonym(a) for a in t]\n",
    "            #print(syn)\n",
    "            for s in syn[0]:\n",
    "                test = \" \".join(t[1:])\n",
    "                w = s.lower() + ' ' + test\n",
    "                #print('zzz', w)\n",
    "                L.append(w)\n",
    "    #print(L.__len__())\n",
    "    L = set(L)\n",
    "    #print(L.__len__())\n",
    "    #write_list_to_txt(L,root + '/'+ filname+'_syno.txt')\n",
    "    return list(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voc_concept = '/home/rodrique/Bureau/Jupyter-notebook/Protocole_3M/doc_collect_adapted/keywords/agriculture.txt'\n",
    "# len(list(generate_new_terms(voc_concept)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage\n",
    "#data = '/home/rodrique/PycharmProjects/Corpus/config/keywords_1500/urbanisation.txt'\n",
    "#generate_new_terms(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_best_doc(voc_concept, doc):\n",
    "    extent_voc_concept = generate_new_terms(voc_concept)\n",
    "    doc = score_doc_sim(extent_voc_concept, doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voc_concept = '/home/rodrique/Bureau/Jupyter-notebook/Protocole_3M/doc_collect_adapted/keywords/agriculture.txt'\n",
    "# doc = {'text' : \"C'est particulièrement net quand les idées ou initiatives à promouvoir viennent en décalage ou en opposition avec les formes dominantes du système existant. Par exemple, développer les circuits courts ou les grossistes en opposition à la grande distribution, favoriser l'installation diversifiée dans un contexte de viticulture spécialisée et de difficulté d'installation, savoir quelle forme d'agriculture promouvoir en fonction des multiples intérêts sur le territoire, identifier les réserves d'emplois dans le système alimentaire et soutenir la création d'emplois qualifiés. Tous ces sujets soulèvent des incertitudes ou des controverses sur ce qu'il est souhaitable et pertinent de faire, ici et maintenant.\"}\n",
    "# compute_best_doc(voc_concept,doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
